import logging
import os
import requests
import json
import time
import pytz
import scrapy
import subprocess
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from datetime import datetime

from dotenv import load_dotenv
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.sensors.base_sensor_operator import BaseSensorOperator
from airflow.utils.dates import timedelta
from airflow.utils.decorators import apply_defaults
from dateutil.relativedelta import relativedelta

load_dotenv()


def validate_environment_variables():
    """
    Validates the presence and validity of required environment variables.
    Raises a ValueError if any of the required variables are missing or empty.
    """
    required_variables = [
        "URL", "AIRLINES", "DELAY_THRESHOLD",
        "TIME_TO_DEPARTURE_THRESHOLD", "CANCELLED_FLIGHT_TIME_WINDOW_START",
        "CANCELLED_FLIGHT_TIME_WINDOW_END"
    ]
    for var in required_variables:
        if not os.getenv(var):
            raise ValueError(f"Environment variable {var} is missing or empty")


class FlightScraperManager:
    def __init__(self):
        self.call_immediately = True

    def get_next_call_time(self, ongoing_delays, next_flight_time, current_time):
        delay_threshold = int(os.getenv('DELAY_THRESHOLD', 1))
        time_to_departure_threshold = int(os.getenv('TIME_TO_DEPARTURE_THRESHOLD', 1))

        if self.call_immediately or any(delay >= delay_threshold and time_to_departure > time_to_departure_threshold
                                        for delay, time_to_departure in ongoing_delays):
            self.call_immediately = False
            return current_time
        elif next_flight_time is not None:
            return next_flight_time + timedelta(minutes=delay_threshold)
        else:
            return current_time + timedelta(minutes=60)


class DelayedScrapyCallSensor(BaseSensorOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.last_scrapy_call_time = datetime.min
        self.flight_scraper_manager = FlightScraperManager()

    def poke(self, context):
        ongoing_delays = context['ti'].xcom_pull(task_ids='analyze_delays_task', key='ongoing_delays') or []
        next_flight_time = context['ti'].xcom_pull(task_ids='analyze_delays_task', key='next_flight_time')
        current_time = datetime.now()

        next_call_time = self.flight_scraper_manager.get_next_call_time(ongoing_delays, next_flight_time, current_time)

        if current_time >= next_call_time:
            self.last_scrapy_call_time = current_time  # Record the last successful Scrapy execution time
            return True

        return False


class FlightSpider(scrapy.Spider):
    name = "flight_spider"

    def start_requests(self):
        urls = os.getenv("URL").split(",")  # Split URLs by comma
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        flight_data = []

        rows = response.css(".flight-row")

        for row in rows:
            destination = row.css(".flight-col__dest b::text").get()
            departure = row.css(".flight-col__hour::text").get()
            flight_numbers = row.css(".flight-col__flight a::text").getall()  # Handle multiple flight numbers
            airlines = row.css(".flight-col__airline a::text").getall()  # Handle multiple airlines
            status = row.css(".flight-col__status a::text").get()

            if destination and departure and flight_numbers and airlines and status:
                # Extract the first flight number from the list
                first_flight_number = flight_numbers[0] if flight_numbers else None

                # Extract the first name of the airline from the list
                first_airline_name = airlines[0].split()[0] if airlines else None

                # Filter out specific statuses
                if status.strip() not in ["En Route [+]", "Landed - Delayed [+]", "Landed - On-time [+]",
                                          "Scheduled - On-time [+]"]:
                    flight_info = {
                        "Destination": destination.strip(),
                        "Departure": departure.strip(),
                        "FlightNumber": first_flight_number.strip() if first_flight_number else None,
                        "Airline": first_airline_name.strip() if first_airline_name else None,
                        "Status": status.strip()
                    }

                    flight_data.append(flight_info)

        # Save flight data to JSON file
        filename = "flight_data.json"
        with open(filename, 'w') as f:
            json.dump(flight_data, f, indent=4)

        self.log("Flight data saved to JSON file: %s" % filename)

        # Filter and display the required flight information
        for flight in flight_data:
            destination = flight["Destination"]
            departure = flight["Departure"]
            flight_number = flight["FlightNumber"]
            airline = flight["Airline"]
            status = flight["Status"]

            if destination:
                print("Destination:", destination)
            if departure:
                print("Departure:", departure)
            if flight_number:
                print("Flight Number:", flight_number)
            if airline:
                print("Airline:", airline)
            if status:
                print("Status:", status)
            print()


class FlightStatusMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.delayInfo = []
        try:
            self.confirm_environment_variables()
            self.retrieve_flight_information()
        except Exception as exception:
            self.logger.error(f"An error occurred during the initialization of FlightStatusMonitor: {str(exception)}")
            raise

    def retrieve_flight_information(self):
        spider_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'FlightSpider.py')
        process = subprocess.Popen(['scrapy', 'runspider', spider_file], stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        out, err = process.communicate()
        print("Stdout:", out)
        print("Stderr:", err)

    def fetch_flight_data(self):
        """
        Fetches the flight data by running the scrapy spider.
        Returns a list of flight data.
        """
        logging.basicConfig(level=logging.INFO)

        process = CrawlerProcess(get_project_settings())
        process.crawl(FlightSpider)
        process.start()

        flight_data = []
        try:
            with open("flight_data.json", 'r') as file:
                flight_data = json.load(file)
        except json.JSONDecodeError:
            logging.error("Invalid or empty JSON file: flight_data.json")

        return flight_data

    def write_flight_data_to_json(self, filename):
        """
        Writes the flight data to a JSON file.
        """
        flight_data = self.fetch_flight_data()

        with open(filename, 'w') as file:
            json.dump(flight_data, file, indent=4)

        logging.info(f"Flight data saved to JSON file: {filename}")

    def confirm_environment_variables(self):
        necessary_env_variables = ["URL"]

        for variable in necessary_env_variables:
            if os.getenv(variable) is None:
                raise ValueError(f"The environment variable {variable} is not defined")

    def analyze_delays(self):
        pass  # Removed analyze_delays method


flight_checker = FlightStatusMonitor()
flight_checker.write_flight_data_to_json("flight_data.json")

default_args = {
    'start_date': datetime.strptime(os.getenv("DAG_START_DATE", "2023-05-21"), "%Y-%m-%d"),
    'retries': int(os.getenv("DAG_RETRIES", "1")),
    'retry_delay': timedelta(minutes=int(os.getenv("DAG_RETRY_DELAY", "1"))),
    'catchup': False,
}

# We set the schedule_interval to None, so that the scheduler does not trigger it, but it can still be manually triggered.
with DAG(
    'flight_checker',
    default_args=default_args,
    description='Flight Checker DAG',
    schedule_interval=None,  # Set this to None
) as dag:
    delayed_scrapy_call_sensor = DelayedScrapyCallSensor(
        task_id='delayed_scrapy_call_sensor',
        dag=dag,
    )

    load_flight_data_task = PythonOperator(
        task_id='load_flight_data_task',
        python_callable=flight_checker.fetch_flight_data,
        provide_context=True,
        dag=dag,
    )

    analyze_delays_task = PythonOperator(
        task_id='analyze_delays_task',
        python_callable=flight_checker.analyze_delays,
        provide_context=True,
        dag=dag,
    )

    delayed_scrapy_call_sensor >> load_flight_data_task >> analyze_delays_task
