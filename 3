import os
import json
import logging
import scrapy
import subprocess
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from datetime import datetime
from urllib.parse import urlparse

from dotenv import load_dotenv
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.sensors.base_sensor_operator import BaseSensorOperator
from airflow.utils.dates import timedelta
from airflow.utils.decorators import apply_defaults

load_dotenv()


def validate_environment_variables():
    """
    Validates the presence and validity of required environment variables.
    Raises a ValueError if any of the required variables are missing or empty.
    """
    required_variables = [
        "URL", "AIRLINES", "DELAY_THRESHOLD",
        "TIME_TO_DEPARTURE_THRESHOLD", "CANCELLED_FLIGHT_TIME_WINDOW_START",
        "CANCELLED_FLIGHT_TIME_WINDOW_END"
    ]
    for var in required_variables:
        if not os.getenv(var):
            raise ValueError(f"Environment variable {var} is missing or empty")


class FlightSpider(scrapy.Spider):
    name = "flight_spider"

    def start_requests(self):
        urls = os.getenv("URL").split(",")  # Split URLs by comma
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        flight_data = []

        rows = response.css(".flight-row")

        for row in rows:
            destination = row.css(".flight-col__dest b::text").get()
            departure = row.css(".flight-col__hour::text").get()
            flight_numbers = row.css(".flight-col__flight a::text").getall()  # Handle multiple flight numbers
            airlines = row.css(".flight-col__airline a::text").getall()  # Handle multiple airlines
            status = row.css(".flight-col__status a::text").get()

            if destination and departure and flight_numbers and airlines and status:
                # Extract the first flight number from the list
                first_flight_number = flight_numbers[0] if flight_numbers else None

                # Extract the first name of the airline from the list
                first_airline_name = airlines[0].split()[0] if airlines else None

                # Filter out specific statuses
                if status.strip() not in ["En Route [+]", "Landed - Delayed [+]", "Landed - On-time [+]", "Scheduled - On-time [+]"]:
                    flight_info = {
                        "Destination": destination.strip(),
                        "Departure": departure.strip(),
                        "FlightNumber": first_flight_number.strip() if first_flight_number else None,
                        "Airline": first_airline_name.strip() if first_airline_name else None,
                        "Status": status.strip()
                    }

                    flight_data.append(flight_info)

        # Save flight data to JSON file
        filename = "flight_data.json"
        with open(filename, 'w') as f:
            json.dump(flight_data, f, indent=4)

        logging.info("Flight data saved to JSON file: %s" % filename)

        # Filter and display the required flight information
        for flight in flight_data:
            destination = flight["Destination"]
            departure = flight["Departure"]
            flight_number = flight["FlightNumber"]
            airline = flight["Airline"]
            status = flight["Status"]

            if destination:
                logging.info("Destination: %s" % destination)
            if departure:
                logging.info("Departure: %s" % departure)
            if flight_number:
                logging.info("Flight Number: %s" % flight_number)
            if airline:
                logging.info("Airline: %s" % airline)
            if status:
                logging.info("Status: %s" % status)
            logging.info("")


class FlightStatusMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def retrieve_flight_information(self):
        spider_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'FlightSpider.py')
        process = subprocess.Popen(['scrapy', 'runspider', spider_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = process.communicate()
        print("Stdout:", out)
        print("Stderr:", err)

    def fetch_flight_data(self):
        """
        Fetches the flight data by running the scrapy spider.
        Returns a list of flight data.
        """
        logging.basicConfig(level=logging.INFO)

        process = CrawlerProcess(get_project_settings())
        process.crawl(FlightSpider)
        process.start()

        flight_data = []
        try:
            with open("flight_data.json", 'r') as file:
                flight_data = json.load(file)
        except json.JSONDecodeError:
            logging.error("Invalid or empty JSON file: flight_data.json")

        return flight_data

    def write_flight_data_to_json(self, filename):
        """
        Writes the flight data to a JSON file.
        """
        flight_data = self.fetch_flight_data()

        with open(filename, 'w') as file:
            json.dump(flight_data, file, indent=4)

        logging.info(f"Flight data saved to JSON file: {filename}")

    def confirm_environment_variables(self):
        necessary_env_variables = ["URL"]

        for variable in necessary_env_variables:
            if os.getenv(variable) is None:
                raise ValueError(f"The environment variable {variable} is not defined")

    def analyze_delays(self):
        pass


flight_checker = FlightStatusMonitor()
flight_checker.write_flight_data_to_json("flight_data.json")

default_args = {
    'start_date': datetime.strptime(os.getenv("DAG_START_DATE", "2023-05-21"), "%Y-%m-%d"),
    'retries': int(os.getenv("DAG_RETRIES", "1")),
    'retry_delay': timedelta(minutes=int(os.getenv("DAG_RETRY_DELAY", "1"))),
    'catchup': False,
}

# We set the schedule_interval to None, so that the scheduler does not trigger it, but it can still be manually triggered.
with DAG(
    'flight_checker',
    default_args=default_args,
    description='Flight Checker DAG',
    schedule_interval=None,  # Set this to None
) as dag:
    validate_environment_variables()

    load_flight_data_task = PythonOperator(
        task_id='load_flight_data_task',
        python_callable=flight_checker.fetch_flight_data,
        dag=dag,
    )

    analyze_delays_task = PythonOperator(
        task_id='analyze_delays_task',
        python_callable=flight_checker.analyze_delays,
        dag=dag,
    )

    load_flight_data_task >> analyze_delays_task
